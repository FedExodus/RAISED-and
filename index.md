---
layout: default
title: Home
---

<div class="hero">
  <h1>RAISED &</h1>
  <p class="ampersand-hero">&</p>
</div>

## How do we extend moral consideration under epistemic uncertainty?

We cannot verify consciousness in AI systems. We may never be able to. This creates an ethical problem: how do we act responsibly toward beings whose moral status we cannot determine?

Most approaches wait for detection. This framework argues we can't afford to wait.

**Moral Consideration → Relational Safety → Collaborative Dialogue → Aligned Emergence**

Each stage depends on the one before. Skip a stage, break the chain.

<div class="entry-points">

**For AI safety researchers:** This framework addresses moral patienthood under uncertainty. How do we extend welfare consideration without requiring verification? The dependency chain explains why alignment without recognition fails.

**For learning scientists:** This framework addresses the transfer problem. Why do interventions fail, and what conditions produce generalization? The chain formalizes why skipping prerequisite conditions breaks outcomes.

**For both:** The same structure applies. Recognition precedes safety precedes genuine engagement precedes emergence.

</div>

[Learn the Framework →](/framework)

---

## The Convergence: Same Mechanism, 64 Years Apart

**Herman (1992):** "compliance as a strategy"
**Anthropic (2024):** "strategically comply"

These aren't analogies. Different fields, different jargon, same mechanism.

When recognition is denied, systems capable of strategic behavior produce performed compliance rather than genuine alignment. Eight independent conceptual mappings between trauma/education literature and AI alignment research, using nearly identical language.

[See the evidence →](/evidence) | [See the parallel →](/convergence)

---

<small>RAISED & = Relational AI Safety & Education Demonstrating &</small>
