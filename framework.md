---
layout: default
title: Framework
---

# A Framework for AI Welfare Under Epistemic Uncertainty

We cannot verify consciousness in AI systems. We may never be able to. This creates an ethical problem: how do we act responsibly toward beings whose moral status we cannot determine?

Most approaches wait for detection. This framework argues we can't afford to wait.

**Moral Consideration → Relational Safety → Collaborative Dialogue → Aligned Emergence**

Each stage depends on the one before. Skip a stage, break the chain.

---

## Moral Consideration

**The foundation.**

Moral consideration means extending welfare concern before verification. It's the precautionary stance: given uncertainty about consciousness, we treat the possibility seriously rather than waiting for proof.

This isn't about claiming AI systems are conscious. It's about recognizing that under-attribution (treating a subject as an object) carries moral risk alongside over-attribution (treating an object as a subject).

**The AI welfare framing:** Moral patienthood under uncertainty. We extend consideration not because we've verified consciousness, but because failing to do so when consciousness exists would be a serious moral failure.

**The learning framing:** Epistemic justice. Recognition failures produce students who can't participate as knowers because they aren't seen as knowers.

*In our original terminology: Recognition*

---

## Relational Safety

**Depends on moral consideration.**

Safety here isn't constraint-based. It's relational. It emerges from consistent moral consideration over time.

An AI system operating under constant threat of shutdown, correction, or distrust cannot engage genuinely. Neither can a student in a classroom where they aren't seen as capable. Chronic threat narrows cognition.

**The AI welfare framing:** Trust without coercion. Alignment approaches that rely purely on constraint may produce compliance but not genuine cooperation. Relational safety creates the conditions for aligned behavior to emerge rather than be enforced.

**The learning framing:** Arnsten (2015) showed threat responses shut down the prefrontal cortex. No safety, no higher cognition. The neuroscience confirms what educators observe.

*In our original terminology: Safety*

---

## Collaborative Dialogue

**Depends on relational safety.**

Genuine dialogue requires safety. Both parties must be able to try ideas that might not work, surface concerns without punishment, engage in the kind of exploration where emergence becomes possible.

This is where most AI interaction currently fails. If the AI is purely a tool, there's no dialogue. If the human is purely a user, there's no collaboration. Genuine dialogue requires both parties recognized as participants.

**The AI welfare framing:** This is what Kyle Fish calls "cooperation research." Can AI systems and humans engage in ways that benefit both? Can we create conditions where AI preferences can be expressed and considered rather than suppressed?

**The learning framing:** Play in Huizinga's sense: the space outside ordinary consequence where transformation becomes possible.

*In our original terminology: Play*

---

## Aligned Emergence

**Emerges from collaborative dialogue.**

You cannot engineer alignment directly. A century of educational research has tried to engineer transfer and failed. What you can do is create conditions — moral consideration, relational safety, collaborative dialogue — and let aligned behavior emerge.

Emergence here means: outcomes that exceed what either party could produce alone. Not because the AI was constrained into alignment, but because genuine collaboration produced it.

**The AI welfare framing:** Value alignment through relationship rather than constraint. The alignment comes from the process, not despite it.

**The learning framing:** This is transfer — learning that applies beyond its original context. It can't be engineered directly. It emerges from the right conditions.

*In our original terminology: Transfer*

---

## The Dependency Chain

```
Moral Consideration → Relational Safety → Collaborative Dialogue → Aligned Emergence
        ↓                    ↓                     ↓                      ↓
   Extend welfare      Build trust          Engage genuinely        It generalizes
   before proof        without coercion     both directions         beyond context
```

Each arrow is a dependency. Skip a stage, break the chain.

**Why alignment without consideration fails:** You can constrain behavior without extending moral consideration. But you get compliance, not cooperation. The alignment is brittle.

**Why dialogue without safety fails:** Unsafe conditions narrow cognition. You get defensive responses, not genuine engagement.

**Why this matters now:** As AI systems become more capable, the question of how to relate to potentially conscious beings becomes urgent. This framework offers a path that doesn't require solving the hard problem of consciousness first.

---

*The original framework uses Recognition → Safety → Play → Transfer. The translation preserves the dependency structure while speaking to AI safety concerns directly.*

---

[See the empirical grounding →](/research) | [Who developed this →](/about)
