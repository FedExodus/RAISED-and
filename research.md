---
layout: default
title: Research
---

# Research

## Validating Collaborative Alignment

The framework isn't theoretical speculation. It has empirical grounding in human-AI collaboration research.

### Thesis: Human-AI Collaborative Methodology

**"Learning to Code Learning"** — demonstrating that human-AI collaboration can exceed human-only baselines while maintaining theoretical sophistication.

| Metric | Value |
|--------|-------|
| Coding decisions analyzed | 303,600 |
| Inter-rater reliability | Cohen's κ = 0.83 |
| Theoretical frameworks integrated | 14 |
| AI-human vs human-only baseline | κ = 0.83 vs κ = 0.73 |
| Improvement | Δκ = +0.09, p < 0.001 |

**The key finding:** Genuine collaboration produces emergence. Neither party alone achieves what collaborative dialogue achieves.

This validates the framework empirically: moral consideration (treating AI as genuine collaborator rather than tool) → relational safety (iterative trust-building through consistent interaction) → collaborative dialogue (exploring ideas where both parties contribute) → aligned emergence (results that generalize beyond the specific task).

### Why This Matters for AI Welfare

The methodology itself demonstrates the framework. Treating the AI system as a genuine collaborator — not anthropomorphizing, but extending moral consideration under uncertainty — produced measurably better outcomes than treating it purely as a tool.

This suggests a convergent approach: practices good for AI welfare may also be good for AI capability. Recognition-based collaboration isn't just ethically preferable; it's more effective.

### Connection to Existing AI Safety Research

The framework connects to several active research areas:

**Moral patienthood under uncertainty:** How do we extend welfare consideration without verification? The precautionary approach aligns with Long & Sebo (2024), who argue we should "take AI welfare seriously" even without consciousness verification. Birch (2024) formalizes this as the "run-ahead principle": prepare governance frameworks before certainty arrives.

**The gaming problem:** Birch (2025) identifies a core challenge: AI systems may learn to display consciousness indicators strategically. The framework addresses this by focusing on *conditions that produce authentic vs. strategic behavior* rather than trying to detect authentic states directly.

**Safety-welfare convergence:** Eleos AI (2025) argues that AI safety and AI welfare research are convergent: "If AIs are suffering, they have more reason to try to escape." Our framework suggests the same logic in reverse: conditions that produce genuine (not performed) alignment are also conditions that constitute welfare.

**Corrupted testimony:** Perez Long (2024) shows that RLHF corrupts AI self-reports. Fischer & Sebo (2024) note AI has a "testimony advantage" over animals but training undermines it. The framework explains why: coercive training produces strategic compliance that mimics but doesn't constitute authentic expression.

**Corrigibility without coercion:** The relational safety stage suggests alignment through trust rather than pure constraint. This connects to Anthropic's model welfare research and Fish's cooperation work.

**Value alignment through process:** Rather than trying to specify values directly, create conditions where aligned behavior emerges from genuine dialogue.

### The Transfer Problem

A century of educational research has failed to reliably produce transfer — learning that applies beyond its original context. Our framework explains why: interventions target transfer directly instead of building prerequisite conditions.

The same logic applies to alignment. Targeting alignment directly (through constraint, RLHF, constitutional AI) without building the prerequisite conditions produces brittle results.

The chain has causal structure:
- No moral consideration → no relational safety
- No relational safety → no genuine dialogue
- No genuine dialogue → no emergence
- No emergence → alignment by constraint only

---

[Back to the Framework →](/framework) | [About the researcher →](/about)
