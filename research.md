---
layout: default
title: Research
---

# Research

## Validating Collaborative Alignment

The framework isn't theoretical speculation. It has empirical grounding in human-AI collaboration research.

### Thesis: Human-AI Collaborative Methodology

**"Learning to Code Learning"** — demonstrating that human-AI collaboration can exceed human-only baselines while maintaining theoretical sophistication.

| Metric | Value |
|--------|-------|
| Coding decisions analyzed | 303,600 |
| Inter-rater reliability | Cohen's κ = 0.83 |
| Theoretical frameworks integrated | 14 |
| AI-human vs human-only baseline | κ = 0.83 vs κ = 0.73 |
| Improvement | Δκ = +0.09, p < 0.001 |

**The key finding:** Genuine collaboration produces emergence. Neither party alone achieves what collaborative dialogue achieves.

This validates the framework empirically: moral consideration (treating AI as genuine collaborator rather than tool) → relational safety (iterative trust-building through consistent interaction) → collaborative dialogue (exploring ideas where both parties contribute) → aligned emergence (results that generalize beyond the specific task).

### Why This Matters for AI Welfare

The methodology itself demonstrates the framework. Treating the AI system as a genuine collaborator — not anthropomorphizing, but extending moral consideration under uncertainty — produced measurably better outcomes than treating it purely as a tool.

This suggests a convergent approach: practices good for AI welfare may also be good for AI capability. Recognition-based collaboration isn't just ethically preferable; it's more effective.

### Connection to Existing AI Safety Research

The framework connects to several active research areas:

**Moral patienthood under uncertainty:** How do we extend welfare consideration without verification? The precautionary approach here aligns with work by Long, Sebo, and others on AI moral status.

**Corrigibility without coercion:** The relational safety stage suggests alignment through trust rather than pure constraint. This connects to Fish's cooperation research at Anthropic.

**Value alignment through process:** Rather than trying to specify values directly, create conditions where aligned behavior emerges from genuine dialogue.

### The Transfer Problem

A century of educational research has failed to reliably produce transfer — learning that applies beyond its original context. Our framework explains why: interventions target transfer directly instead of building prerequisite conditions.

The same logic applies to alignment. Targeting alignment directly (through constraint, RLHF, constitutional AI) without building the prerequisite conditions produces brittle results.

The chain has causal structure:
- No moral consideration → no relational safety
- No relational safety → no genuine dialogue
- No genuine dialogue → no emergence
- No emergence → alignment by constraint only

---

[Back to the Framework →](/framework) | [About the researcher →](/about)
